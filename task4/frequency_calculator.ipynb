{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d00c7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bbaaa75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод получения списка слов (в нижнем регистре) из текста\n",
    "def get_words_from_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words = words + [word.lower() for word in word_tokenize(sentence)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ecdec895",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files_indices_with_words = {}\n",
    "total_texts_count = 0\n",
    "data_dir_path = '../Выкачка/'\n",
    "\n",
    "# Проходим по файлам выкачек, собираем общее количество файлов\n",
    "# Также сохраняем словарь индекс_файла: список слов в файле\n",
    "for filename in os.listdir(data_dir_path):\n",
    "    if 'выкачка' not in filename:\n",
    "        continue\n",
    "    total_texts_count += 1\n",
    "    full_file_path = data_dir_path + filename\n",
    "    with open(full_file_path, 'r') as f:\n",
    "        text_files_indices_with_words[total_texts_count] = get_words_from_text(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ced26f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод подсчета tf для термина = word в тексте файла\n",
    "def calculate_token_tf(text_index, word):\n",
    "    text_words = text_files_indices_with_words[text_index]\n",
    "    word_occurence_count = text_words.count(word.lower())\n",
    "    return word_occurence_count / len(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "22c817a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод подсчета idf для термина\n",
    "def calculate_token_idf(word):\n",
    "    word_occurence_in_texts_count = 0\n",
    "    for text_file_index in text_files_indices_with_words:\n",
    "        text_words = text_files_indices_with_words[text_file_index]\n",
    "        if word in text_words:\n",
    "            word_occurence_in_texts_count += 1\n",
    "    return word_occurence_in_texts_count / total_texts_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "125a70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем список всех токенов, собранных при токенизации (см. tokenizer.ipynb)\n",
    "tokens = []\n",
    "with open('../tokens.txt', 'r') as f:\n",
    "    tokens = f.read().split('\\n')\n",
    "    \n",
    "for text_file_index in text_files_indices_with_words:\n",
    "    # Получаем список слов для каждого текста\n",
    "    text_words = text_files_indices_with_words[text_file_index]\n",
    "    token_tf_idf_file_path = '../tf_idf_freq/tokens/tokens_' + str(text_file_index) + '.txt'\n",
    "    # Для каждого токена, если он содержится в файле подсчитываем его параметры tf-idf-tf_idf\n",
    "    # Параметры записываются в соответствующий файл tokens_${file_index}.txt\n",
    "    with open(token_tf_idf_file_path, 'w') as tokens_tf_idf_file:\n",
    "        for word in text_words:\n",
    "            if word not in tokens:\n",
    "                continue\n",
    "            tf = calculate_token_tf(text_file_index, word)\n",
    "            idf = calculate_token_idf(word)\n",
    "            tf_idf = tf * idf\n",
    "            tokens_tf_idf_file.write(word + ' ' + str(tf) + ' ' + str(idf) + ' ' + str(tf_idf) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "41af55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_dict = {}\n",
    "\n",
    "# Получаем словарь инвертированного индекса, полученного в inverted_index.ipynb\n",
    "with open('../inverted_index.txt', 'r') as index_file:\n",
    "    for line in index_file:\n",
    "        line_json = json.loads(line)\n",
    "        inverted_index_dict[line_json['word']] = line_json['inverted_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9e1a3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = {}\n",
    "\n",
    "# Получаем словарь лемма: список токенов, полученных в результате лемматизации в tokenizer.ipynb\n",
    "with open('../lemmas.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        lemma_with_tokens = line.split(':')\n",
    "        lemma = lemma_with_tokens[0]\n",
    "        words = [word.strip() for word in lemma_with_tokens[1].split(' ') if word != '']\n",
    "        lemmas[lemma] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c2885db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод подсчета tf для леммы\n",
    "def calculate_lemma_tf(text_index, lemma):\n",
    "    lemma_words = lemmas[lemma]\n",
    "    lemma_count = 0\n",
    "    for word in lemma_words:\n",
    "        lemma_count += calculate_token_tf(text_index, word)\n",
    "    return lemma_count / len(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b87272e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод подсчета idf для леммы\n",
    "def calculate_lemma_idf(lemma):\n",
    "    lemma_files_indices = inverted_index_dict[lemma]\n",
    "    return len(lemma_files_indices) / total_texts_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ba2f4309",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_file_index in text_files_indices_with_words:\n",
    "    lemma_tf_idf_file_path = '../tf_idf_freq/lemmas/lemmas_' + str(text_file_index) + '.txt'\n",
    "    # Для каждой леммы, если она содержится в файле подсчитываем ее параметры tf-idf-tf_idf\n",
    "    # Параметры записываются в соответствующий файл lemmas_${file_index}.txt\n",
    "    with open(lemma_tf_idf_file_path, 'w') as lemmas_tf_idf_file:\n",
    "        for lemma in lemmas:\n",
    "            if text_file_index in inverted_index_dict[lemma]:\n",
    "                tf = calculate_lemma_tf(text_file_index, lemma)\n",
    "                idf = calculate_token_idf(lemma)\n",
    "                tf_idf = tf * idf\n",
    "                lemmas_tf_idf_file.write(lemma + ' ' + str(tf) + ' ' + str(idf) + ' ' + str(tf_idf) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be79d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
