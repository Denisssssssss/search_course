{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16dc847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json\n",
    "\n",
    "# Для запуска необходимо развернуть на локальной машине Stanford CoreNLP\n",
    "# https://nlp.stanford.edu/software/stanford-corenlp-latest.zip\n",
    "\n",
    "# Команда для запуска:\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"tokenize,ssplit,pos,lemma,parse,sentiment\" -port 9000 -timeout 30000\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000, timeout=30000)\n",
    "props = {'annotators': 'pos,lemma',\n",
    "         'pipelineLanguage': 'en',\n",
    "         'outputFormat': 'json'}\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ccfe065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lemmas = {}\n",
    "    words = []\n",
    "    # Разделяем полученный текст на предложения\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        # Разделяем предложение на отдельные слова-токены\n",
    "        tokens = word_tokenize(sentence)\n",
    "        # Очищаем предложение от стоп-слов и символов пунктуации\n",
    "        cleared_sentence = ' '.join([word.lower() for word in tokens if word.isalpha() and word not in stop_words])\n",
    "        # Для пустых строк нет необходимости вычислять леммы\n",
    "        if not cleared_sentence.strip():\n",
    "            continue\n",
    "        # С помощью Stanford CoreNLP вычисляем параметры предложения в виде JSON\n",
    "        parsed_str = nlp.annotate(cleared_sentence, properties=props)\n",
    "        parsed_dict = json.loads(parsed_str)\n",
    "        for word_data in parsed_dict['sentences'][0]['tokens']:\n",
    "            # Получаем оригинальное слово из предложения\n",
    "            sentence_word = word_data['originalText'].lower()\n",
    "            # Получаем лемму слова\n",
    "            word_lemma = word_data['lemma']\n",
    "            # Заполняем список токенов/словарь лемм\n",
    "            words.append(sentence_word)\n",
    "            if word_lemma in lemmas:\n",
    "                lemmas[word_lemma] = lemmas[word_lemma] + [sentence_word]\n",
    "            else:\n",
    "                lemmas[word_lemma] = [sentence_word]\n",
    "    return words, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cb3ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод для склеивания двух словарей\n",
    "def merge_dicts(dict1, dict2):\n",
    "    for key in dict2:\n",
    "        if key in dict1:\n",
    "            dict1[key] = dict1[key] + dict2[key]\n",
    "        else:\n",
    "            dict1[key] = dict2[key]\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51a7a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод записи лемм в файл\n",
    "def write_lemmas_to_file(file_path, lemmas_dict):\n",
    "    with open(file_path, 'w') as lemmas_file:\n",
    "        for key in lemmas_dict:\n",
    "            lemma_str = key + ': ' + ' '.join([val for val in set(lemmas_dict[key])])\n",
    "            lemmas_file.write(lemma_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f0531db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод записи токенов в файл\n",
    "def write_tokens_to_file(file_path, tokens):\n",
    "    with open(file_path, 'w') as tokens_file:\n",
    "        tokens_str = '\\n'.join([val for val in tokens])\n",
    "        tokens_file.write(tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cafcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir_path = '../Выкачка/'\n",
    "data_file_pattern = 'выкачка'\n",
    "lemmas = {}\n",
    "tokens = []\n",
    "\n",
    "# Для каждого файла выкачки собираем токены и леммы и склеиваем в общий список для записи в файлы\n",
    "for filename in os.listdir(data_dir_path):\n",
    "    if data_file_pattern not in filename:\n",
    "        continue\n",
    "    file_path = data_dir_path + filename\n",
    "    with open(file_path, 'r') as text_file:\n",
    "        file_tokens, file_lemmas = tokenize(text_file.read())\n",
    "        lemmas = merge_dicts(lemmas, file_lemmas)\n",
    "        tokens = tokens + file_tokens\n",
    "        \n",
    "write_tokens_to_file('../tokens.txt', tokens)\n",
    "write_lemmas_to_file('../lemmas.txt', lemmas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
